<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>blog of meurice</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://meurice.xyz/"/>
  <updated>2021-01-08T15:52:06.110Z</updated>
  <id>http://meurice.xyz/</id>
  
  <author>
    <name>meurice</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[学习笔记]Meta Learning(元学习)</title>
    <link href="http://meurice.xyz/2021/ckjmunc2x00007glxchngch50/"/>
    <id>http://meurice.xyz/2021/ckjmunc2x00007glxchngch50/</id>
    <published>2021-01-07T12:51:38.000Z</published>
    <updated>2021-01-08T15:52:06.110Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>　　Meta Learning = Learn to learn<br>　　Meta：How to learn a new model</p><h2 id="Meta-Learning"><a href="#Meta-Learning" class="headerlink" title="Meta Learning"></a>Meta Learning</h2><p>　　Meta Learning即“学会学习”，学习了一些task后，机器学会如何去学习新的task，例如机器学习了task1——语音辨识，task2——图像辨识，…，然后给一个新的task（例如文本分类），面对这个新的task，机器能够快速的学习。<br><img src= "/img/loading.gif" data-src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/meta_learning/0.png" alt><br>　　其步骤和Machine Learning类似，其中不同的是Maching Learning需要找到一个Function <strong>f</strong>，而Meta Learning需要找到的是一个Learning algorithm <strong>F</strong>：<br>　　step1. define a set of learning algorithm<br>　　step2. goodness of learning algorithm<br>　　step3. pick the best learnDing algorithm<br><img src= "/img/loading.gif" data-src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/meta_learning/1.png" alt></p><h3 id="step1-Define-a-set-of-learning-algorithm"><a href="#step1-Define-a-set-of-learning-algorithm" class="headerlink" title="step1. Define a set of learning algorithm"></a>step1. Define a set of learning algorithm</h3><p>　　首先准备训练资料，其为一堆训练数据D和一堆f的集合，对于每一个task来说，整个流程构成的不再是像Machine Learning中的参数θ，而是构成了一个f，即每当使用新的参数进行初始化时，我们定义了一个新的f。<br><img src= "/img/loading.gif" data-src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/meta_learning/2.png" alt></p><h3 id="step2-Define-the-goodness-of-a-Function-F"><a href="#step2-Define-the-goodness-of-a-Function-F" class="headerlink" title="step2. Define the goodness of a Function F"></a>step2. Define the goodness of a Function F</h3><p>　　对于每一个task，都能得到f，并且有损失l。<br><img src= "/img/loading.gif" data-src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/meta_learning/3.png" alt><br>　　N：N tasks<br>　　l^n：Testing loss for task n after training<br><img src= "/img/loading.gif" data-src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/meta_learning/4.png" alt></p><h2 id="Omniglot——Few-shot-Classification"><a href="#Omniglot——Few-shot-Classification" class="headerlink" title="Omniglot——Few-shot Classification"></a>Omniglot——Few-shot Classification</h2><p>　　Omniglot 数据集总共包含 5050 个字母。我们通常将这些分成一组包含 3030 个字母的背景（background）集和一组包含 2020 个字母的评估（evaluation）集。  </p><h3 id="N-ways-K-shot"><a href="#N-ways-K-shot" class="headerlink" title="N-ways K-shot"></a>N-ways K-shot</h3><p>　　N-ways K-shot： In each training and test tasks, there are N classes, each has K examples.<br>  <img src= "/img/loading.gif" data-src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/meta_learning/5.png" alt="example"></p><h2 id="MAML"><a href="#MAML" class="headerlink" title="MAML"></a>MAML</h2><p>　　MAML的基本思想是：对于每一个task中学到的f，其仅决定参数的赋值方式，而不决定模型架构等内容，网络结构是提前固定的。  </p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>  <img src= "/img/loading.gif" data-src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/meta_learning/6.png" alt>  </p><p>　　最小化L(Φ)：<strong>Gradient Descent</strong><br>  <img src= "/img/loading.gif" data-src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/meta_learning/7.png" alt></p><h3 id="MAML对比Model-Pre-training"><a href="#MAML对比Model-Pre-training" class="headerlink" title="MAML对比Model Pre-training"></a>MAML对比Model Pre-training</h3><p>　　MAML不在意Φ在train task上的表现如何，而是在意用Φ训练出的参数θ hat ^ n表现如何。<br>　　而Model Pre-training希望找到在task1和task2上损失都最小的Φ。<br>  <img src= "/img/loading.gif" data-src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/meta_learning/10.png" alt><br>　　如下图，可能Φ在task1和task2上表现都不太好，但假设拿这个Φ做初始参数，对于task1和task2来说，都能比较容易的找到最佳参数。<br>  <img src= "/img/loading.gif" data-src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/meta_learning/8.png" alt><br>　　而对于Model Pre-training来说，希望找到的是如下图所示的这个Φ，但不保证拿这个Φ去训练后能得到好的θ hat ^ n。<br>  <img src= "/img/loading.gif" data-src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/meta_learning/9.png" alt><br>　　MAML中参数仅被update一次后就被当作最终的参数。其一原因是需要获得的结果参数量过大，多次更新带来的时间成本大大增加。另一原因是MAML的训练目标是训练后得到非常好的Init，希望更新一次后就能得到非常好的效果，一般训练时只更新一次参数，但测试的时候更新多次。<br>  <img src= "/img/loading.gif" data-src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/meta_learning/11.png" alt><br>  <img src= "/img/loading.gif" data-src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/meta_learning/12.png" alt></p><h2 id="Reptile"><a href="#Reptile" class="headerlink" title="Reptile"></a>Reptile</h2><h2 id="Gradient-Descent-as-LSTM"><a href="#Gradient-Descent-as-LSTM" class="headerlink" title="Gradient Descent as LSTM"></a>Gradient Descent as LSTM</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;　　Meta Learning = Learn to lea
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>中移集成（雄安产业研究院）首届OneCity编程大赛</title>
    <link href="http://meurice.xyz/2020/ckguib4rx0000tclx0cw25v45/"/>
    <id>http://meurice.xyz/2020/ckguib4rx0000tclx0cw25v45/</id>
    <published>2020-12-03T04:33:49.000Z</published>
    <updated>2020-12-30T11:04:50.549Z</updated>
    
    <content type="html"><![CDATA[<h2 id="赛题任务"><a href="#赛题任务" class="headerlink" title="赛题任务"></a>赛题任务</h2><p>　　选手需要建立模型，针对政务表格文件实现自动化分类。允许使用一些常见的开源预训练模型，如bert。<br>　　数据智能分类按照行业领域，将政务数据分为以下20个基础大类，分别是：生态环境、资源能源、信息产业、医疗卫生、文化休闲、财税金融、经济管理、教育科技、交通运输、工业、农业畜牧业、政法监察、城乡建设、商业贸易、旅游服务、气象水文测绘地震地理、外交外事、文秘行政、民政社区、劳动人事。</p><h2 id="初赛方案"><a href="#初赛方案" class="headerlink" title="初赛方案"></a>初赛方案</h2><p>　　参赛时赛事日程已过半，提交次数较少，仅考虑了一些简单的方案。<br>　　初赛阶段，文件标题基本完整，首先仅使用标题进行训练，预训练模型使用了RoBERTa-wwm-ext，使用五折CV。<br>　　读取文件content后，针对content单独训练bert，最后将title和content的raw_output按0.7、0.3加权平均，线上accuracy 0.985，rank 13。</p><h2 id="决赛方案"><a href="#决赛方案" class="headerlink" title="决赛方案"></a>决赛方案</h2><p>　　舍弃初赛方案——Bert训练标题+TextCNN训练Content，直接使用TextCNN训练标题+Content拼接后的内容，并且最终acc指标受文本长度影响较大。<br>　　训练前做了一定量的数据预处理工作，包括关键字提取、地理位置提取等，其中省市提取使用了<a href="https://github.com/DQinYuan/chinese_province_city_area_mapper" target="_blank" rel="noopener">cpca</a>。<br>　　由于花了大量的时间用于文件读取上，导致预处理不够深入，而预处理也是本题的一个关键上分点，此外还可以对网络增加例如文件长度等特征输入。最终决赛线上rank20+。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;赛题任务&quot;&gt;&lt;a href=&quot;#赛题任务&quot; class=&quot;headerlink&quot; title=&quot;赛题任务&quot;&gt;&lt;/a&gt;赛题任务&lt;/h2&gt;&lt;p&gt;　　选手需要建立模型，针对政务表格文件实现自动化分类。允许使用一些常见的开源预训练模型，如bert。&lt;br&gt;　　数据智能分类
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>第三届全国高校绿色计算大赛半决赛——OpenEuler（第一阶段）</title>
    <link href="http://meurice.xyz/2020/ckgnffb760000t0lx0csre2zg/"/>
    <id>http://meurice.xyz/2020/ckgnffb760000t0lx0csre2zg/</id>
    <published>2020-10-24T08:33:55.000Z</published>
    <updated>2020-10-29T06:16:05.014Z</updated>
    
    <content type="html"><![CDATA[<h2 id="OpenEuler-、-Shell-编程"><a href="#OpenEuler-、-Shell-编程" class="headerlink" title="OpenEuler 、 Shell 编程"></a>OpenEuler 、 Shell 编程</h2><h3 id="1-正则替换"><a href="#1-正则替换" class="headerlink" title="1.正则替换"></a>1.正则替换</h3><p>系统提供了一个待处理的test.txt文件，其内容为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ZhangSan25man</span><br><span class="line">LiSi7man</span><br><span class="line">WangWu16woman</span><br><span class="line">Alex77man</span><br></pre></td></tr></table></figure><p>请在右侧的编码区域，通过shell脚本将文件内容中的三列用 ,(英文逗号) 分割开来。其中，第一列为人名，由字母组成；第二列为年龄，由数字组成；第三列为性别，只会是 man 或者 woman。  </p><p>处理完成后预期结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">ZhangSan,25,man</span><br><span class="line">LiSi,7,man</span><br><span class="line">WangWu,16,woman</span><br><span class="line">Alex,77,man</span><br></pre></td></tr></table></figure><p>Solve：<br>正则表达式匹配字母+任意位数字+字母，分别以\1，\2，\3引用匹配到的内容，添加逗号即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat test.txt | sed "s/\([a-z]\)\([0-9]*[0-9]\)\([a-z]\)/\1,\2,\3/g"</span><br></pre></td></tr></table></figure><h3 id="2-求集合差集"><a href="#2-求集合差集" class="headerlink" title="2.求集合差集"></a>2.求集合差集</h3><p>系统提供了两个待处理文件a.txt和b.txt，其中文件 a.txt 中的部分内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Hello</span><br><span class="line">My Name is Alice</span><br><span class="line">What is your name</span><br><span class="line">I am Bob</span><br><span class="line">I came from China</span><br><span class="line">Where are you from</span><br><span class="line">Oh my God</span><br></pre></td></tr></table></figure><p>文件 b.txt 中的部分内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Alice is a good boy</span><br><span class="line">Bob is a nice man and he is one of my best friend</span><br><span class="line">God bless you</span><br></pre></td></tr></table></figure><p>将文件 a.txt 中每一行的最后一个单词作为集合 1 ；将文件 b.txt 中每一行的第一个单词作为集合 2 ；请使用 shell 语言编写程序，输出包含在集合 1 但不包含在集合 2 的所有元素。<br>Solve：通过正则将a.txt中最后一个空格前的全部字符替换为空，b.txt中第一个空格后的全部字符替换为空，sort后通过uniq命令解决。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">sed -i &#39;s&#x2F;.* &#x2F;&#x2F;g&#39; a.txt</span><br><span class="line">sed -i &#39;s&#x2F; .*&#x2F;&#x2F;g&#39; b.txt</span><br><span class="line">grep -F -v -f b.txt a.txt | sort | uniq</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;OpenEuler-、-Shell-编程&quot;&gt;&lt;a href=&quot;#OpenEuler-、-Shell-编程&quot; class=&quot;headerlink&quot; title=&quot;OpenEuler 、 Shell 编程&quot;&gt;&lt;/a&gt;OpenEuler 、 Shell 编程&lt;/h2&gt;&lt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>[学习日志]智算之道——人工智能应用挑战赛</title>
    <link href="http://meurice.xyz/2020/ckg5cnsiq0004jklx8jiwcppw/"/>
    <id>http://meurice.xyz/2020/ckg5cnsiq0004jklx8jiwcppw/</id>
    <published>2020-10-10T02:01:16.000Z</published>
    <updated>2021-01-08T04:14:55.983Z</updated>
    
    <content type="html"><![CDATA[<h2 id="空值填充"><a href="#空值填充" class="headerlink" title="空值填充"></a>空值填充</h2><p>　　以众数填充为例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> column <span class="keyword">in</span> list(all_data.columns[all_data.isnull().sum() &gt; <span class="number">0</span>]):</span><br><span class="line">  mode_val = all_data[column].mode()[<span class="number">0</span>]</span><br><span class="line">  all_data[column].fillna(mode_val, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>　　注意可能有多个众数，一般取第一个（mode()[0]），否则填充后的DataFrame仍存在空值。</p><h2 id="focal-loss-——tensorflow实现"><a href="#focal-loss-——tensorflow实现" class="headerlink" title="focal loss ——tensorflow实现"></a>focal loss ——tensorflow实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">focal_loss</span><span class="params">(logits, labels, gamma=<span class="number">2</span>)</span>:</span></span><br><span class="line">    softmax = tf.reshape(tf.nn.softmax(logits), [<span class="number">-1</span>])</span><br><span class="line">    labels = tf.range(<span class="number">0</span>, tf.shape(logits)[<span class="number">0</span>]) * tf.shape(logits)[<span class="number">1</span>] + labels</span><br><span class="line">    prob = tf.gather(softmax, labels)</span><br><span class="line">    weight = tf.pow(tf.subtract(<span class="number">1.</span>, prob), gamma)</span><br><span class="line">    loss = -tf.reduce_mean(tf.multiply(weight, tf.log(prob)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h2 id="初赛解决方案"><a href="#初赛解决方案" class="headerlink" title="初赛解决方案"></a>初赛解决方案</h2><p>　　原始特征 + 交叉特征 + 整体(train+test)空值填充，lgb 5折 + catboost，Averaging加权0.4 / 0.6，线上auc 0.8581，个人rank4。<br>　　尝试过FiBiNET + Facol Loss，结果不够稳定，随机种子对结果有一定影响，初赛算力有限，无法做过多特征，线上auc约0.856</p><h2 id="Function-call-stack-train-function"><a href="#Function-call-stack-train-function" class="headerlink" title="Function call stack: train_function"></a>Function call stack: train_function</h2><p>　　在tensorflow2.x下调用tf.combat1没有关闭eager_execution()，加入如下代码解决：<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.compat.v1.disable_eager_execution()</span><br></pre></td></tr></table></figure></p><h2 id="fit-generator"><a href="#fit-generator" class="headerlink" title="fit_generator"></a>fit_generator</h2><p>　　从tensorflow 2.1.0开始已不推荐使用fit_generator，fit替代之。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">(batch_size)</span>:</span></span><br><span class="line">    j = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        x_train = np.zeros((batch_size, <span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">            img_path= <span class="string">'./'</span></span><br><span class="line">            img = cv2.imread(img_path)</span><br><span class="line">            img = cv2.resize(img,(<span class="number">128</span>,<span class="number">128</span>))</span><br><span class="line">            x_train[i] = img</span><br><span class="line">        labels = y_train[(j<span class="number">-1</span>)*batch_size:j*batch_size]</span><br><span class="line">        j=j+<span class="number">1</span></span><br><span class="line">        <span class="keyword">yield</span> x_train, labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit_generator</span></span><br><span class="line">fit_generator(self, </span><br><span class="line">generator, </span><br><span class="line">            steps_per_epoch,</span><br><span class="line">            epochs=<span class="number">1</span>, </span><br><span class="line">            verbose=<span class="number">1</span>, </span><br><span class="line">            callbacks=<span class="literal">None</span>, </span><br><span class="line">            validation_data=<span class="literal">None</span>, </span><br><span class="line">            validation_steps=<span class="literal">None</span>, </span><br><span class="line">            class_weight=<span class="literal">None</span>, </span><br><span class="line">            max_q_size=<span class="number">10</span>, </span><br><span class="line">            workers=<span class="number">1</span>, </span><br><span class="line">            pickle_safe=<span class="literal">False</span>, </span><br><span class="line">            initial_epoch=<span class="number">0</span></span><br><span class="line">            )</span><br></pre></td></tr></table></figure><h2 id="决赛解决方案"><a href="#决赛解决方案" class="headerlink" title="决赛解决方案"></a>决赛解决方案</h2><p>　　基线模型为EfficientNET-b5，使用20%的数据预热后，冻结除Dense外的所有层并使用全量数据调整网络，可以参考NeurIPS 2019所收录的一篇文章——<a href="http://papers.nips.cc/paper/9035-fixing-the-train-test-resolution-discrepancy" target="_blank" rel="noopener">Fixing the train-test resolution discrepancy</a>。<br> 　　调用模型对测试集分类结果进行推断时，对测试集做了测试时增强（test time augmentation, TTA），此部分内容可参考<a href="https://github.com/ultralytics/yolov5/issues/303" target="_blank" rel="noopener">Test-Time Augmentation (TTA) Tutorial</a>，包括随机裁切、左右翻转等（考虑天气图像的特殊性，未做上下翻转）。<br>   <img src= "/img/loading.gif" data-src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%99%BA%E7%AE%97%E4%B9%8B%E9%81%932020/scheme.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;空值填充&quot;&gt;&lt;a href=&quot;#空值填充&quot; class=&quot;headerlink&quot; title=&quot;空值填充&quot;&gt;&lt;/a&gt;空值填充&lt;/h2&gt;&lt;p&gt;　　以众数填充为例。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020 计蒜之道 预赛 第一场</title>
    <link href="http://meurice.xyz/2020/ckg5cnsim0001jklx1a9c6wdg/"/>
    <id>http://meurice.xyz/2020/ckg5cnsim0001jklx1a9c6wdg/</id>
    <published>2020-09-20T12:11:32.000Z</published>
    <updated>2020-09-24T05:43:54.194Z</updated>
    
    <content type="html"><![CDATA[<h2 id="A-五子棋"><a href="#A-五子棋" class="headerlink" title="A.五子棋"></a>A.五子棋</h2><p>　　题意：给出当前状态棋盘，判断下一步黑棋是否可以获胜，并输出下在哪里可获胜。<br>　　思路：遍历棋盘每一处，判断每个位置能否获胜，<strong>需要判断先手方</strong>。<br>  <img src= "/img/loading.gif" data-src="https://wx2.sbimg.cn/2020/09/24/GzuD8.png" alt=""></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//solution</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> IO_OP std::ios::sync_with_stdio(0); std::cin.tie(0);</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> F first</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> S second</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> V vector</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PB push_back</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MP make_pair</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> EB emplace_back</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ALL(v) (v).begin(), (v).end()</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> debug(x) cerr <span class="meta-string">&lt;&lt; "Line(" &lt;&lt; __LINE__ &lt;&lt; ") -&gt; " &lt;&lt; #x &lt;&lt; " is " &lt;&lt; x &lt;&lt; endl</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">long</span> <span class="keyword">long</span> ll;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; pi;</span><br><span class="line"><span class="keyword">typedef</span> V&lt;<span class="keyword">int</span>&gt; vi;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> INF = <span class="number">1e9</span> + <span class="number">7</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">char</span> a[<span class="number">25</span>][<span class="number">25</span>], who;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">yes</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">20</span>; i++) &#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">25</span>; j++) &#123;</span><br><span class="line"><span class="keyword">bool</span> ok = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; <span class="number">5</span>; k++)</span><br><span class="line"><span class="keyword">if</span>(a[i+k][j]!=who) ok = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">if</span>(ok) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">25</span>; i++) &#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= <span class="number">20</span>; j++) &#123;</span><br><span class="line"><span class="keyword">bool</span> ok = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; <span class="number">5</span>; k++)</span><br><span class="line"><span class="keyword">if</span>(a[i][j+k]!=who) ok = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">if</span>(ok) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">20</span>; i++) &#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= <span class="number">20</span>; j++) &#123;</span><br><span class="line"><span class="keyword">bool</span> ok = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; <span class="number">5</span>; k++)</span><br><span class="line"><span class="keyword">if</span>(a[i+k][j+k]!=who) ok = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">if</span>(ok) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">4</span>; i &lt; <span class="number">25</span>; i++) &#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= <span class="number">20</span>; j++) &#123;</span><br><span class="line"><span class="keyword">bool</span> ok = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; <span class="number">5</span>; k++)</span><br><span class="line"><span class="keyword">if</span>(a[i-k][j+k]!=who) ok = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">if</span>(ok) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">signed</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">IO_OP;</span><br><span class="line"><span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">25</span>; i++)</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">25</span>; j++) &#123;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; a[i][j];</span><br><span class="line"><span class="keyword">if</span>(a[i][j] != <span class="string">'.'</span>) cnt++;</span><br><span class="line">&#125;</span><br><span class="line">who = <span class="string">'x'</span>;</span><br><span class="line"><span class="keyword">if</span>(cnt&amp;<span class="number">1</span>) who = <span class="string">'o'</span>;</span><br><span class="line"><span class="keyword">bool</span> y = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">25</span>; i++) &#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">25</span>; j++) &#123;</span><br><span class="line"><span class="keyword">if</span>(a[i][j] == <span class="string">'.'</span>) &#123;</span><br><span class="line">a[i][j] = who;</span><br><span class="line"><span class="keyword">if</span>(yes()) &#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; i &lt;&lt; <span class="string">" "</span> &lt;&lt; j &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">y = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line">a[i][j] = <span class="string">'.'</span>; </span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span>(!y) <span class="built_in">cout</span> &lt;&lt; <span class="string">"tie"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="B-染色-简单"><a href="#B-染色-简单" class="headerlink" title="B.染色(简单)"></a>B.染色(简单)</h2><p>　　题意：序列中的第 i 个位置染成黑色会产生 b_i 的美感，染成白色会产生 w_i 的美感。有些区间比较特殊，如果区间内的所有数都染成黑色会额外得到 c_i 的美感；另一些区间则恰好相反，如果区间内的所有数都染成白色会额外得到 c_i 的美感。求美感总和最大值。<br>　　思路：<br>  <img src= "/img/loading.gif" data-src="https://wx2.sbimg.cn/2020/09/24/Gzq8m.png" alt=""></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> IO_OP std::ios::sync_with_stdio(0); std::cin.tie(0);</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> F first</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> S second</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> V vector</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PB push_back</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MP make_pair</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> EB emplace_back</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ALL(v) (v).begin(), (v).end()</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> debug(x) cerr <span class="meta-string">&lt;&lt; "Line(" &lt;&lt; __LINE__ &lt;&lt; ") -&gt; " &lt;&lt; #x &lt;&lt; " is " &lt;&lt; x &lt;&lt; endl</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> int ll</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">long</span> <span class="keyword">long</span> ll;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; pi;</span><br><span class="line"><span class="keyword">typedef</span> V&lt;<span class="keyword">int</span>&gt; vi;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> INF = <span class="number">1e9</span> + <span class="number">7</span>, N = <span class="number">3e5</span> + <span class="number">7</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> b[N], w[N], bsum[N], wsum[N], mx[N], sum[N], yes[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">signed</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">IO_OP;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> n, m;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; n &gt;&gt; m;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; b[i];</span><br><span class="line">bsum[i] = bsum[i - <span class="number">1</span>] + b[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; w[i];</span><br><span class="line">wsum[i] = wsum[i - <span class="number">1</span>] + w[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">mx[i] = <span class="built_in">max</span>(b[i], w[i]);</span><br><span class="line">sum[i] = mx[i] + sum[i - <span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= m; i++) &#123;</span><br><span class="line"><span class="keyword">int</span> t, l, r, c;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; t &gt;&gt; l &gt;&gt; r &gt;&gt; c;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = l; j &lt;= r; j++) yes[j] = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">int</span> cur = sum[r] - sum[l<span class="number">-1</span>];</span><br><span class="line"><span class="keyword">if</span>(t == <span class="number">1</span>) &#123;</span><br><span class="line">cur = <span class="built_in">max</span>(cur, bsum[r] - bsum[l<span class="number">-1</span>] + c);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">cur = <span class="built_in">max</span>(cur, wsum[r] - wsum[l<span class="number">-1</span>] + c);</span><br><span class="line">&#125;</span><br><span class="line">ans += cur;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++) <span class="keyword">if</span>(!yes[i]) ans += mx[i];</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; ans &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;A-五子棋&quot;&gt;&lt;a href=&quot;#A-五子棋&quot; class=&quot;headerlink&quot; title=&quot;A.五子棋&quot;&gt;&lt;/a&gt;A.五子棋&lt;/h2&gt;&lt;p&gt;　　题意：给出当前状态棋盘，判断下一步黑棋是否可以获胜，并输出下在哪里可获胜。&lt;br&gt;　　思路：遍历棋盘每一处，判断
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>[学习日志]2020 DIGIX全球校园AI算法精英大赛——赛道B</title>
    <link href="http://meurice.xyz/2020/ckg5cnu3a000ajklx7wih8vpi/"/>
    <id>http://meurice.xyz/2020/ckg5cnu3a000ajklx7wih8vpi/</id>
    <published>2020-07-28T10:06:11.000Z</published>
    <updated>2020-08-11T15:52:20.529Z</updated>
    
    <content type="html"><![CDATA[<h2 id="2020-7-28"><a href="#2020-7-28" class="headerlink" title="2020.7.28"></a>2020.7.28</h2><h3 id="Resnet50预训练权重文件"><a href="#Resnet50预训练权重文件" class="headerlink" title="Resnet50预训练权重文件"></a>Resnet50预训练权重文件</h3><p>　　.h5文件已上传至百度网盘，链接放在此处。<br>   　　<a href="https://pan.baidu.com/s/1jTn1lI101BZfOoFys9tlOA" target="_blank" rel="noopener">resnet50_weights_tf_dim_ordering_tf_kernels.h5</a>，提取码: pdcg<br>      　　放在C://users//(yourusername)//.keras//models文件下。<br>         　　另外，可以通过<a href="https://d.serctl.com/" target="_blank" rel="noopener">该网站</a>下载Github上的release内容。</p><h3 id="plt-imshow与cv2-imshow显示色差"><a href="#plt-imshow与cv2-imshow显示色差" class="headerlink" title="plt.imshow与cv2.imshow显示色差"></a>plt.imshow与cv2.imshow显示色差</h3><p>　　使用plt.imshow和cv2.imshow对同一幅图显示时，可能会出现色差，这是由于opencv的接口为BGR，而matplotlib.pyplot接口使用的是RGB。<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)</span><br><span class="line"></span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br>　　或通过以下方法也可实现：<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b,g,r &#x3D; cv2.split(cv2.imread(img_path))</span><br><span class="line">img &#x3D; cv2.merge([r,g,b])</span><br></pre></td></tr></table></figure></p><h2 id="2020-8-7"><a href="#2020-8-7" class="headerlink" title="2020.8.7"></a>2020.8.7</h2><h3 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h3><p>　　余弦相似性通过测量两个向量的夹角的余弦值来度量它们之间的相似性。0度角的余弦值是1，而其他任何角度的余弦值都不大于1；并且其最小值是-1。从而两个向量之间的角度的余弦值确定两个向量是否大致指向相同的方向。两个向量有相同的指向时，余弦相似度的值为1；两个向量夹角为90°时，余弦相似度的值为0；两个向量指向完全相反的方向时，余弦相似度的值为-1。该结果仅与向量方向相关。余弦相似度通常用于正空间，因此给出的值为-1到1之间。<br>　　<img src= "/img/loading.gif" data-src="https://wx2.sbimg.cn/2020/08/08/oJscK.png" alt=""><br>　　给定两个属性向量，A和B，其余弦相似性θ由点积和向量长度给出：<br>　　<img src= "/img/loading.gif" data-src="https://wx2.sbimg.cn/2020/08/08/oJLVT.png" alt=""><br>　　对于两个向量的<strong>余弦距离</strong>（余弦距离 = 1 - 余弦相似度）的基本计算，Python代码如下：<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosin_distance</span><span class="params">(vec_1, vec_2)</span>:</span></span><br><span class="line">  dot_product = <span class="number">0.0</span></span><br><span class="line">  normA = <span class="number">0.0</span></span><br><span class="line">  normB = <span class="number">0.0</span></span><br><span class="line">  <span class="keyword">for</span> a, b <span class="keyword">in</span> zip(vec_1, vec_2):</span><br><span class="line">      dot_product += a * b</span><br><span class="line">      normA += a ** <span class="number">2</span></span><br><span class="line">      normB += b ** <span class="number">2</span></span><br><span class="line">  <span class="keyword">if</span> normA == <span class="number">0.0</span> <span class="keyword">or</span> normB == <span class="number">0.0</span>:</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> dot_product / ((normA * normB) ** <span class="number">0.5</span>)</span><br></pre></td></tr></table></figure></p><h2 id="2020-8-8"><a href="#2020-8-8" class="headerlink" title="2020.8.8"></a>2020.8.8</h2><h3 id="大规模数据下使用faiss计算余弦相似度-待完善"><a href="#大规模数据下使用faiss计算余弦相似度-待完善" class="headerlink" title="大规模数据下使用faiss计算余弦相似度(待完善)"></a>大规模数据下使用faiss计算余弦相似度(待完善)</h3>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">d = <span class="number">2048</span>                           <span class="comment"># dimension</span></span><br><span class="line"></span><br><span class="line">nb = gallery_features.shape[<span class="number">0</span>]        <span class="comment"># database size</span></span><br><span class="line">nq = query_features.shape[<span class="number">0</span>]      <span class="comment"># nb of queries</span></span><br><span class="line"></span><br><span class="line">xb = gallery_features.astype(<span class="string">'float32'</span>)</span><br><span class="line">xq = query_features.astype(<span class="string">'float32'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">nlist = <span class="number">1000</span>                      <span class="comment">#聚类中心的个数</span></span><br><span class="line">k = <span class="number">10</span>      <span class="comment"># topk搜索</span></span><br><span class="line">quantizer = faiss.IndexFlatL2(d)  <span class="comment"># the other index</span></span><br><span class="line">index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)</span><br><span class="line">     <span class="comment"># here we specify METRIC_L2, by default it performs inner-product search</span></span><br><span class="line"><span class="keyword">assert</span> <span class="keyword">not</span> index.is_trained</span><br><span class="line">index.train(xb)</span><br><span class="line"><span class="keyword">assert</span> index.is_trained</span><br><span class="line"> </span><br><span class="line">index.add(xb)                  <span class="comment"># add may be a bit slower as well</span></span><br><span class="line">D, I = index.search(xq, k)     <span class="comment"># actual search</span></span><br><span class="line">index.nprobe = <span class="number">10</span>              <span class="comment"># default nprobe is 1, try a few more</span></span><br><span class="line">D, I = index.search(xq, k)</span><br></pre></td></tr></table></figure><p>　　此处参考<a href="https://github.com/facebookresearch/faiss/wiki/Getting-started" target="_blank" rel="noopener">官方样例</a>。</p><h2 id="2020-8-11"><a href="#2020-8-11" class="headerlink" title="2020.8.11"></a>2020.8.11</h2><h3 id="Keras添加网络结构报错"><a href="#Keras添加网络结构报错" class="headerlink" title="Keras添加网络结构报错"></a>Keras添加网络结构报错</h3>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(load_model(<span class="string">'/mnt/resnet.model'</span>).get_output_at(<span class="number">0</span>))</span><br></pre></td></tr></table></figure><p>　　<em>TypeError: The added layer must be an instance of class Layer.</em><br>　　可能是混合使用了keras.Sequential()和tf.keras.Sequential()；Keras的layer中有input和output属性，错误地使用该部分的成员函数时也可能导致该问题。<br>　　修改如下：<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(load_model(<span class="string">'/mnt/resnet.model'</span>).get_layer(index=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>　　</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;2020-7-28&quot;&gt;&lt;a href=&quot;#2020-7-28&quot; class=&quot;headerlink&quot; title=&quot;2020.7.28&quot;&gt;&lt;/a&gt;2020.7.28&lt;/h2&gt;&lt;h3 id=&quot;Resnet50预训练权重文件&quot;&gt;&lt;a href=&quot;#Resnet50预训
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>特征工程</title>
    <link href="http://meurice.xyz/2020/ckg5cnu3c000bjklxaclk5yp7/"/>
    <id>http://meurice.xyz/2020/ckg5cnu3c000bjklxaclk5yp7/</id>
    <published>2020-07-18T03:42:42.000Z</published>
    <updated>2020-09-06T12:23:12.169Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言　　"></a>前言　　</h2><p>　　数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。</p><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>　　特征工程是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用，简单来说，就是通过X，创造新的X’，目的是去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系，其本质是一个表示和展现数据的过程。基本的操作包括，衍生（升维），筛选（降维）等。<br>　　例如某分类器接收身高、体重两个参数来判断这个人是否肥胖，仅通过体重无法判断某个人的胖瘦，对于该例，一个非常经典的特征工程是，BMI指数，BMI=体重/(身高^2)，通过BMI指数，可以清晰地对一个人的胖瘦进行刻画。  </p><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>　　常见的数据可分为结构化数据（例如关系型数据库的表）和非结构化数据（文本、图像、音频、视频等）。</p><h3 id="单特征"><a href="#单特征" class="headerlink" title="单特征"></a>单特征</h3><h4 id="标准化与归一化"><a href="#标准化与归一化" class="headerlink" title="标准化与归一化"></a>标准化与归一化</h4><p>　　该部分可以参考<a href="http://meurice.xyz/2020/ckcqevh3t0004xclxakyx24ma/">数据预处理——归一化与标准化</a>。</p><h4 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h4><h5 id="均值-中位数-众数-固定值填充"><a href="#均值-中位数-众数-固定值填充" class="headerlink" title="均值/中位数/众数/固定值填充"></a>均值/中位数/众数/固定值填充</h5><p>　　如果样本属性的距离是可度量的，则使用该属性有效值的平均值来补全；如果样本属性的距离不可度量，则可以采用众数或者中位数来补全。<br>　　或可根据某一特征对样本进行分类/聚合后（例如船运GPS数据，根据运单号进行聚合后，对样本数据缺失值进行填充），根据同类其他样本该属性的均值补全缺失值，同上述方法类似。<br>　　对于缺失值也可以采用固定的数值来进行填充。</p><h5 id="建模预测"><a href="#建模预测" class="headerlink" title="建模预测"></a>建模预测</h5><p>　　将缺失值字段作为预测对象，建立模型对其进行预测，根据该模型补全原训练集的缺失值。这个方法根本的缺陷是如果其他属性和缺失属性无关，则预测的结果毫无意义；但若模型对预测字段拟合效果相当好，则说明这个缺失属性没必要纳入数据集；一般的情况是介于两者之间。</p><h5 id="高维映射"><a href="#高维映射" class="headerlink" title="高维映射"></a>高维映射</h5><p>　　将属性映射到高维空间，采用独热码编码（one-hot）技术。将包含 K 个离散取值范围的属性值扩展为 K+1 个属性值，若该属性值缺失，则扩展后的第 K+1 个属性值置为 1。<br>　　这种做法既保留了所有的信息，也未添加任何额外信息，但会增加数据的维度，增大了计算量，一般在样本量非常大时效果才比较好。</p><h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><p>　　如多重插补、压缩感知和矩阵补全等，此处不具体展开，可以参考<a href="https://mp.weixin.qq.com/s/BnTXjzHSb5-4s0O0WuZYlg" target="_blank" rel="noopener">这篇文章</a>。</p><h4 id="特征二值化"><a href="#特征二值化" class="headerlink" title="特征二值化"></a>特征二值化</h4><p>　　 设立阈值，将特征二值化。<br>　　<img src= "/img/loading.gif" data-src="https://wx1.sbimg.cn/2020/07/18/ClGSk.png" alt="erzhihua"><br>　　可以类比将模拟信号转换成数字信号过程中的量化。<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_ = preprocessing.Binarizer(threshold=<span class="number">0</span>).transform(X)</span><br></pre></td></tr></table></figure></p><h4 id="哑编码-独热编码"><a href="#哑编码-独热编码" class="headerlink" title="哑编码/独热编码"></a>哑编码/独热编码</h4><p>　　哑编码/独热编码针对定性的特征进行处理。</p><h5 id="哑编码-dummy-encoding"><a href="#哑编码-dummy-encoding" class="headerlink" title="哑编码(dummy encoding)"></a>哑编码(dummy encoding)</h5><p>　　假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。<br>　　例如描述一个人的身材，我们可以用偏瘦、正常、偏胖，这些描述词经过哑编码就会得到：<br>　　　　偏廋 —&gt; [1, 0, 0]<br>　　　　正常 —&gt; [0, 1, 0]<br>　　　　偏胖 —&gt; [0, 0, 1]<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_ = pd.Categorical(df[<span class="string">'c'</span>]).codes</span><br></pre></td></tr></table></figure></p><h5 id="独热编码-one-hot-encoding"><a href="#独热编码-one-hot-encoding" class="headerlink" title="独热编码(one-hot encoding)"></a>独热编码(one-hot encoding)</h5><p>　　同上例，实际用2个状态位就足够反应上述3个类别的信息：<br>　　　　偏廋 —&gt; [1, 0]<br>　　　　正常 —&gt; [0, 1]<br>　　　　偏胖 —&gt; [0, 0]<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">encoder=OneHotEncoder(sparse=<span class="literal">False</span>) </span><br><span class="line"><span class="comment"># sparse是一个布尔值，指定结果是否稀疏。</span></span><br><span class="line"><span class="comment"># 若sparse=True，则每个样本的独热码为一个稀疏矩阵。</span></span><br></pre></td></tr></table></figure><br>  <br><br>　　关于哑编码/独热编码的区别和联系以及连续值的离散化提升模型的非线性能力的原因，可以参考<a href="https://www.cnblogs.com/lianyingteng/p/7792693.html" target="_blank" rel="noopener">这篇文章</a>。</p><h3 id="多特征"><a href="#多特征" class="headerlink" title="多特征"></a>多特征</h3><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p>　　数据预处理完成后，需要选择有意义的特征输入机器学习的算法和模型进行训练，一般从以下两个方面考虑：<br>　　· 特征是否发散（某特征不发散，说明对于区分样本作用并不大）<br>　　· 特征与目标的相关性  </p><p>　　特征选择主要包括：<br>　　· Filter Method （过滤式）<br>　　· Wrapper Method （包装式）<br>　　· Embedded Method （嵌入式）</p><h5 id="特征选择原理"><a href="#特征选择原理" class="headerlink" title="特征选择原理"></a>特征选择原理</h5><p>　　·去除无关特征可以降低学习任务的难度，也同样让模型变得简单，降低计算复杂度　　</p><h5 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h5><p>　　过滤式方法先对数据集进行特征选择，然后再训练模型，<strong>特征选择过程与后续模型训练无关</strong>。<br>　　通过统计学的方法对每个feature给出一个score，通过score对特征进行排序，然后选取score最高的子集.。这种方法仅仅对每个feature进行<strong>独立考虑</strong>，没有考虑到feture之间的依赖性或相关性。  </p><h6 id="方差选择法"><a href="#方差选择法" class="headerlink" title="方差选择法"></a>方差选择法</h6><p>　　计算各个特征的方差，根据阈值，<strong>选择方差大于阈值的特征</strong>。即若样本中该特征差异并不大，则认为该特征对于区分样本贡献不大，故可以将其去掉。<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"></span><br><span class="line">VarianceThreshold(threshold=<span class="number">0</span>).fit_transform(data)</span><br></pre></td></tr></table></figure></p><h6 id="相关系数法"><a href="#相关系数法" class="headerlink" title="相关系数法"></a>相关系数法</h6><p>　　计算各个特征对目标值的相关系数以及相关系数的P值。<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"></span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(map(<span class="keyword">lambda</span> x:pearsonr(x, Y), X.T)).T, </span><br><span class="line">            k=<span class="number">4</span>).fit_transform(data, target)</span><br><span class="line"><span class="comment"># 第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。（在此定义为计算相关系数）</span></span><br><span class="line"><span class="comment"># 参数k为选择的特征个数，选择k个最好的特征，返回选择特征后的数据</span></span><br></pre></td></tr></table></figure></p><h6 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h6><p>　　经典的卡方检验是<strong>检验定性自变量对定性因变量的相关性</strong>，是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，如果卡方值越大，二者偏差程度越大；反之，二者偏差越小；若两个值完全相等时，卡方值就为0，表明理论值完全符合。<br>　　假设自变量有N种取值，因变量有M种取值，考虑自变量等于 i 且因变量等于 j 的样本频数的观察值与期望的差距。<br>　　<img src= "/img/loading.gif" data-src="https://wx1.sbimg.cn/2020/07/19/Cyng1.png" alt="x2"><br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2 </span><br><span class="line"><span class="comment"># 选择k个最佳特征</span></span><br><span class="line">SelectKBest(chi2, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure></p><h6 id="互信息法"><a href="#互信息法" class="headerlink" title="互信息法"></a>互信息法</h6><p>　　互信息(Mutual Information)是信息论里一种有用的信息度量，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。<br>　　经典的互信息<strong>评价定性自变量对定性因变量的相关性</strong>。<br>　　设两个随机变量(X, Y)的联合分布为p(x, y)，边缘分布分别为p(x), p(y)，互信息I(X, Y)是联合分布p(x, y)与边缘分布p(x)p(y)的相对熵，即：<br><img src= "/img/loading.gif" data-src="https://wx1.sbimg.cn/2020/07/19/CV0Ek.png" alt="mutual info"><br> 　　关系图：<br><img src= "/img/loading.gif" data-src="https://wx2.sbimg.cn/2020/07/19/CVofa.png" alt="mutual"><br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> minepy <span class="keyword">import</span> MINE</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义mic方法将MINE设为函数式的，返回一个二元组，二元组的第2 项设置成固定的P值0.5</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mic</span><span class="params">(x, y)</span>:</span></span><br><span class="line">m = MINE()</span><br><span class="line">  m.compute_score(x, y)</span><br><span class="line">  <span class="keyword">return</span> (m.mic(), <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(map(<span class="keyword">lambda</span> x:mic(x, Y), X.T)).T,</span><br><span class="line">k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure></p><h5 id="Wrapper"><a href="#Wrapper" class="headerlink" title="Wrapper"></a>Wrapper</h5><p>　　包裹式特征选择直接把最终将要使用的模型的性能作为特征子集的评价标准，即包裹式特征选择的目的就是为给定的模型选择最有利于其性能的特征子集。从最终模型的性能来看，包裹式特征选择比过滤式特征选择更好，但需要多次训练模型，计算开销较大。<br><img src= "/img/loading.gif" data-src="https://wx2.sbimg.cn/2020/07/19/CVCpn.png" alt="filter mutual"></p><h6 id="递归特征消除法"><a href="#递归特征消除法" class="headerlink" title="递归特征消除法"></a>递归特征消除法</h6><p>　　递归特征消除法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处选择LR为基模型(estimator)</span></span><br><span class="line">RFE(estimator=LogisticRegression(), n_features_to_select=<span class="number">4</span>).fit_transform(data, target)</span><br></pre></td></tr></table></figure></p><h5 id="Embedded"><a href="#Embedded" class="headerlink" title="Embedded"></a>Embedded</h5><p>　　在前两种特征选择方法中，特征选择过程和模型训练过程是有明显分别的两个过程。嵌入式特征选择是<strong>将特征选择过程与学习器训练过程融为一体</strong>，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。例如岭回归(Ridge)、LASSO回归。常利用正则化，如L1，L2范数，主要应用于如线性回归、逻辑回归以及支持向量机(SVM)等算法；使用决策树思想，包括决策树、随机森林、Gradient Boosting 等。<br>　　若使用L2范数正则化，则此时优化目标的公式即为岭回归(ridge regression)，若是L1范数正则化，则是LASSO回归(Least Absolute Shrinkage and Selection Operator)。L1范数和L2范数正则化都有助于降低过拟合风险，但前者还会带来一个额外的好处，它比后者更易于获得稀疏解，即它求得的w会有更少的非零分类。换言之，采用L1范数比L2范数更易于得到稀疏解。（参考<a href="https://zhuanlan.zhihu.com/p/120924870" target="_blank" rel="noopener">机器学习（六）：特征选择方法—Filter,Wrapper,Embedded</a>）</p><h6 id="基于惩罚项的特征选择法"><a href="#基于惩罚项的特征选择法" class="headerlink" title="基于惩罚项的特征选择法"></a>基于惩罚项的特征选择法</h6><p>　　使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。<br>　　带L1惩罚项的LR：<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import SelectFromModel</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"> </span><br><span class="line">SelectFromModel(LogisticRegression(penalty&#x3D;&quot;l1&quot;, C&#x3D;0.1)).fit_transform(data, target)</span><br></pre></td></tr></table></figure></p><h6 id="基于树模型的特征选择法"><a href="#基于树模型的特征选择法" class="headerlink" title="基于树模型的特征选择法"></a>基于树模型的特征选择法</h6><p>　　GBDT作为基模型<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"></span><br><span class="line">SelectFromModel(GradientBoostingClassifier()).fit_transform(data, target)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言　　&quot;&gt;&lt;/a&gt;前言　　&lt;/h2&gt;&lt;p&gt;　　数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。&lt;/p&gt;
&lt;h2 id=&quot;特征工程&quot;&gt;&lt;a href=&quot;#特征
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>数据预处理——归一化与标准化</title>
    <link href="http://meurice.xyz/2020/ckg5cnsit0006jklxfxkd5ken/"/>
    <id>http://meurice.xyz/2020/ckg5cnsit0006jklxfxkd5ken/</id>
    <published>2020-07-17T09:33:10.000Z</published>
    <updated>2020-07-18T03:28:29.237Z</updated>
    
    <content type="html"><![CDATA[<p>归一化和标准化都属于四种Feature scaling（特征缩放）方法：<br>　　1.Rescaling(min-max normalization)<br>  <img src= "/img/loading.gif" data-src="https://wx1.sbimg.cn/2020/07/17/C8jCa.png" alt="Rescaling"><br>　　2.Mean normalization<br>  <img src= "/img/loading.gif" data-src="https://wx2.sbimg.cn/2020/07/17/C8h9n.png" alt="Mean normalization"><br>　　3.Standardization(Z-score normalization)<br>  <img src= "/img/loading.gif" data-src="https://wx1.sbimg.cn/2020/07/17/C8swh.png" alt="Standardization"><br>　　4.Scaling to unit length<br>  <img src= "/img/loading.gif" data-src="https://wx2.sbimg.cn/2020/07/17/C8JrM.png" alt="Scaling to unit length"></p><h2 id="归一化-Normalization"><a href="#归一化-Normalization" class="headerlink" title="归一化(Normalization)"></a>归一化(Normalization)</h2><h3 id="归一化目标"><a href="#归一化目标" class="headerlink" title="归一化目标"></a>归一化目标</h3><p>　　归一化将一列数据变化到某个固定区间（范围）中，这一区间通常是[0,1]，广义的讲，可以是各种区间，例如图像中可能会映射到[0,255]。<br>　　归一化使得各个特征维度对目标函数的影响权重是一致的，将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量，同时使得扁平分布的数据伸缩变换成类圆形。<br>　　概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像Adaboost、、xgboost、SVM、LR、KNN、KMeans之类的最优化问题就需要归一化。</p><h3 id="归一化带来的好处"><a href="#归一化带来的好处" class="headerlink" title="归一化带来的好处"></a>归一化带来的好处</h3><p>　　Feature Sacling(Normalization)对基于Gradient descent算法友好，可让算法最终收敛并且提高训练速度和精度。<br>  <img src= "/img/loading.gif" data-src="https://wx2.sbimg.cn/2020/07/17/C8wYT.jpg" alt="scaling"></p><p>　　对于使用梯度下降算法来更新权重的训练过程，每一次更新的delta，除了与学习率有关，还与样本值本身也有关系。<br>  <img src= "/img/loading.gif" data-src="https://wx2.sbimg.cn/2020/07/17/C8vxJ.png" alt="gd"><br>　　Xj(i)就是当前更新批次对应的样本值，因此值越大的样本单次更新权重更快，这就有可能带来收敛速度不一样甚至不收敛等问题。</p><h3 id="归一化算法："><a href="#归一化算法：" class="headerlink" title="归一化算法："></a>归一化算法：</h3><p>　　1.线性转换：x’ = (x - min(x)) / (max(x) - min(x))<br>　　2.对数函数转换：x’ = lg(x) / lg(max)，其中max表示样本数据的最大值，所有样本数据均要大于等于1。<br>　　3.arctan反正切函数转换：x’ = arctan(x) * (2 / pi)，应注意的是，若希望映射的区间为[0,1]，则数据都应该大于等于0，小于0的数据将被映射到[-1,0]区间上。<br>　　4.L2范数归一化：对向量X的每个维度数据x1, x2, …, xn都除以||x||2得到一个新向量，即<br>  <img src= "/img/loading.gif" data-src="https://wx1.sbimg.cn/2020/07/17/C8imm.png" alt="l2"><br>　　经过L2范数归一化后，一组向量的欧式距离和它们的余弦相似度可以等价，严格数学证明可以参考<a href="https://www.cnblogs.com/Kalafinaian/p/11180519.html" target="_blank" rel="noopener">这篇文章</a>。</p><h2 id="标准化-Standardization"><a href="#标准化-Standardization" class="headerlink" title="标准化(Standardization)"></a>标准化(Standardization)</h2><h3 id="标准化目标"><a href="#标准化目标" class="headerlink" title="标准化目标"></a>标准化目标</h3><p>　　将数据变换为均值为0，标准差为1的分布（<strong>不一定是正态分布</strong>）。<br>　　<img src= "/img/loading.gif" data-src="https://wx2.sbimg.cn/2020/07/17/C8FgD.png" alt="st"></p><h2 id="联系与差异"><a href="#联系与差异" class="headerlink" title="联系与差异"></a>联系与差异</h2><h3 id="联系"><a href="#联系" class="headerlink" title="联系"></a>联系</h3><p>　　· Normalization和Standardization都是使数值都落入到统一的数值范围，消除了数据量纲的影响。  </p><h3 id="差异"><a href="#差异" class="headerlink" title="差异"></a>差异</h3><p>　　此处参考文章<a href="https://blog.csdn.net/weixin_36604953/article/details/102652160" target="_blank" rel="noopener">标准化和归一化，请勿混为一谈，透彻理解数据变换</a>。<br>　　· Normalization把数据限定在需要的范围，一般为[0,1]区间；Standardization将数据变换为μ=0，σ=1的分布，但没有严格规定区间。<br>　　· Normalization对数据的缩放比例仅仅和极值有关；但对于Standardization而言，若将除极大值和极小值外的数据更换，则均值和标准差也可能会因此改变，缩放比例也随之改变。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;归一化和标准化都属于四种Feature scaling（特征缩放）方法：&lt;br&gt;　　1.Rescaling(min-max normalization)&lt;br&gt;  &lt;img src= &quot;/img/loading.gif&quot; data-src=&quot;https://wx1.sbim
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Linux设置定时任务</title>
    <link href="http://meurice.xyz/2020/ckg5cnsig0000jklx6btx3bis/"/>
    <id>http://meurice.xyz/2020/ckg5cnsig0000jklx6btx3bis/</id>
    <published>2020-07-16T11:40:44.000Z</published>
    <updated>2020-07-17T14:22:00.702Z</updated>
    
    <content type="html"><![CDATA[<p>环境：CentOS 8</p><h3 id="执行内容"><a href="#执行内容" class="headerlink" title="执行内容"></a>执行内容</h3><p>　　新建文件_crond.sh，作为定时执行的内容。<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">cd &#x2F;www&#x2F;blog&#x2F;hexo</span><br><span class="line">git pull git@github.com:egname&#x2F;egrepo.git</span><br><span class="line"></span><br><span class="line">#echo pull successfully &gt; &#x2F;home&#x2F;gitpull.log</span><br></pre></td></tr></table></figure></p><h3 id="crontab服务"><a href="#crontab服务" class="headerlink" title="crontab服务"></a>crontab服务</h3><p>　　启动crontab服务，CentOS版本不同，具体命令可能有所差异。<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start crond</span><br></pre></td></tr></table></figure><br>　　启动服务</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop crond            # 关闭服务</span><br><span class="line">systemctl restart crond         # 重启服务     </span><br><span class="line">systemctl reload crond          # 重新载入配置</span><br><span class="line">systemctl status crond          # 状态</span><br></pre></td></tr></table></figure><h3 id="设置计时器"><a href="#设置计时器" class="headerlink" title="设置计时器"></a>设置计时器</h3><p>　　crontab 选项 参数<br>　　选项:<br>　　　　-e：编辑该用户的计时器设置；<br>　　　　-l：列出该用户的计时器设置；<br>　　　　-r：删除该用户的计时器设置；<br>　　　　-u：指定要设定计时器的用户名称。<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crontab -e</span><br></pre></td></tr></table></figure></p><p> 　　进入insert插入模式，以每五分钟执行一次为例。ESC后输入wq保存并退出。<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*&#x2F;5 * * * * &#x2F;root&#x2F;_crond.sh</span><br></pre></td></tr></table></figure><br>  <br></p><p>  关于Crontab更多具体用法，您可以参考<a href="https://www.cnblogs.com/muscles/p/9532451.html" target="_blank" rel="noopener">这篇文章</a>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;环境：CentOS 8&lt;/p&gt;
&lt;h3 id=&quot;执行内容&quot;&gt;&lt;a href=&quot;#执行内容&quot; class=&quot;headerlink&quot; title=&quot;执行内容&quot;&gt;&lt;/a&gt;执行内容&lt;/h3&gt;&lt;p&gt;　　新建文件_crond.sh，作为定时执行的内容。&lt;br&gt;  &lt;figure cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>[学习日志]2020 DIGIX全球校园AI算法精英大赛——赛道A</title>
    <link href="http://meurice.xyz/2020/ckg5cnsip0003jklx4zl14fur/"/>
    <id>http://meurice.xyz/2020/ckg5cnsip0003jklx4zl14fur/</id>
    <published>2020-07-16T04:23:28.000Z</published>
    <updated>2020-08-15T04:26:18.076Z</updated>
    
    <content type="html"><![CDATA[<h2 id="2020-7-16"><a href="#2020-7-16" class="headerlink" title="2020.7.16"></a>2020.7.16</h2><p>　　7.20放赛题数据，先拿kaggle五年前的Click-Through Rate Prediction试水。</p><h3 id="分块读取全部数据"><a href="#分块读取全部数据" class="headerlink" title="分块读取全部数据"></a>分块读取全部数据</h3>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">loop = <span class="literal">True</span></span><br><span class="line">chunkSize = <span class="number">1000000</span></span><br><span class="line">chunks = []</span><br><span class="line">index = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> loop:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        print(index)</span><br><span class="line">        chunk = train_data.get_chunk(chunkSize)</span><br><span class="line">        chunks.append(chunk)</span><br><span class="line">        index += <span class="number">1</span></span><br><span class="line">  <span class="keyword">except</span> StopIteration:</span><br><span class="line">        loop = <span class="literal">False</span></span><br><span class="line">        print(<span class="string">"Iteration is stopped."</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(chunks):</span><br><span class="line">    train_data = pd.concat(chunks, ignore_index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="随机读取一定比例的数据"><a href="#随机读取一定比例的数据" class="headerlink" title="随机读取一定比例的数据"></a>随机读取一定比例的数据</h3>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> pd.read_csv(train_data, chunksize=chunksize):</span><br><span class="line">    chunks += <span class="number">1</span></span><br><span class="line">    train = pd.concat([train, chunk.sample(frac=<span class="number">.05</span>, replace=<span class="literal">False</span>, random_state=<span class="number">42</span>)], axis=<span class="number">0</span>) </span><br><span class="line">    print(<span class="string">'Processing Chunk '</span> + str(chunks))</span><br></pre></td></tr></table></figure><h3 id="条件筛选修改"><a href="#条件筛选修改" class="headerlink" title="条件筛选修改"></a>条件筛选修改</h3>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'column_d'</span>].loc[df[<span class="string">'column_c'</span>] == <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"><span class="comment"># df['column_d'][df['column_c'] == 0] = 0</span></span><br></pre></td></tr></table></figure><h3 id="lightgbm-梯度提升决策树"><a href="#lightgbm-梯度提升决策树" class="headerlink" title="lightgbm 梯度提升决策树"></a>lightgbm 梯度提升决策树</h3>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kfold_lightgbm</span><span class="params">(train, test, features, target, seed=<span class="number">42</span>, is_shuffle=True)</span>:</span></span><br><span class="line">   train_pred = np.zeros((train.shape[<span class="number">0</span>],))</span><br><span class="line">   test_pred = np.zeros((test.shape[<span class="number">0</span>],))</span><br><span class="line">   n_splits = <span class="number">5</span>  </span><br><span class="line">   </span><br><span class="line">   fold = KFold(n_splits=n_splits, shuffle=is_shuffle, random_state=seed)</span><br><span class="line">   kf_way = fold.split(train[features])</span><br><span class="line"></span><br><span class="line">   params = &#123;</span><br><span class="line">       <span class="string">'learning_rate'</span>: <span class="number">0.003</span>,</span><br><span class="line">       <span class="string">'boosting_type'</span>: <span class="string">'gbdt'</span>,</span><br><span class="line">       <span class="string">'objective'</span>: <span class="string">'regression'</span>,</span><br><span class="line">       <span class="string">'num_leaves'</span>: <span class="number">36</span>,</span><br><span class="line">       <span class="string">'metric'</span>: <span class="string">'mse'</span>,</span><br><span class="line">       <span class="string">'feature_fraction'</span>: <span class="number">0.6</span>,</span><br><span class="line">       <span class="string">'bagging_fraction'</span>: <span class="number">0.7</span>,</span><br><span class="line">       <span class="string">'bagging_freq'</span>: <span class="number">6</span>,</span><br><span class="line">       <span class="string">'seed'</span>: <span class="number">42</span>,</span><br><span class="line">       <span class="string">'bagging_seed'</span>: <span class="number">1</span>,</span><br><span class="line">       <span class="string">'feature_fraction_seed'</span>: <span class="number">7</span>,</span><br><span class="line">       <span class="string">'min_data_in_leaf'</span>: <span class="number">7</span>,</span><br><span class="line">       <span class="string">'nthread'</span>: <span class="number">8</span>,</span><br><span class="line">       <span class="string">'verbose'</span>: <span class="number">1</span>,</span><br><span class="line">   &#125;</span><br><span class="line">   fold_importance_df = pd.DataFrame()</span><br><span class="line">   <span class="keyword">for</span> n_fold, (train_idx, valid_idx) <span class="keyword">in</span> enumerate(kf_way, start=<span class="number">1</span>):</span><br><span class="line">       train_x, train_y = train[features].iloc[train_idx], train[target].iloc[train_idx]</span><br><span class="line">       valid_x, valid_y = train[features].iloc[valid_idx], train[target].iloc[valid_idx]</span><br><span class="line"></span><br><span class="line">       n_train = lgb.Dataset(train_x, label=train_y)</span><br><span class="line">       n_valid = lgb.Dataset(valid_x, label=valid_y)</span><br><span class="line"></span><br><span class="line">       clf = lgb.train(</span><br><span class="line">           params= params,</span><br><span class="line">           train_set= n_train,</span><br><span class="line">           num_boost_round= <span class="number">10000</span>,</span><br><span class="line">           valid_sets= [n_valid],</span><br><span class="line">           early_stopping_rounds= <span class="number">150</span>,</span><br><span class="line">           verbose_eval= <span class="number">100</span></span><br><span class="line">       )</span><br><span class="line">       train_pred[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration)</span><br><span class="line">       test_pred += clf.predict(test[features], num_iteration=clf.best_iteration) / fold.n_splits</span><br><span class="line"></span><br><span class="line">       fold_importance_df[<span class="string">"Feature"</span>] = features</span><br><span class="line">       fold_importance_df[<span class="string">"importance"</span>] = clf.feature_importance(importance_type=<span class="string">'gain'</span>)</span><br><span class="line">       fold_importance_df[<span class="string">"fold"</span>] = n_splits</span><br><span class="line"></span><br><span class="line">   test[TARGET] = test_pred</span><br><span class="line">   <span class="keyword">return</span> test[[<span class="string">'id'</span>, TARGET]], fold_importance_df</span><br></pre></td></tr></table></figure><h3 id="特征编码"><a href="#特征编码" class="headerlink" title="特征编码"></a>特征编码</h3>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> cat_features:</span><br><span class="line">    ce_oe = ce.OrdinalEncoder(cols=cat_features, handle_unknown=<span class="string">'impute'</span>)</span><br><span class="line">    ce_oe.fit(train)</span><br><span class="line">    train = ce_oe.transform(train)</span><br><span class="line">    test = ce_oe.transform(test)</span><br></pre></td></tr></table></figure><h2 id="2020-7-20"><a href="#2020-7-20" class="headerlink" title="2020.7.20"></a>2020.7.20</h2><p>　　下午官网放了赛题数据，随机抽了5%的数据放进GBDT跑了一下，目测效果并不是很好，CTR标签分布很不均匀，训练集标签为1的样本大概只占到了3%。</p><h3 id="结果出现负值"><a href="#结果出现负值" class="headerlink" title="结果出现负值"></a>结果出现负值</h3><p>　　GBDT是加法模型，下一轮都是上一轮预测值和实际值的残差作为label继续拟合，将结果相加，最后可能会出现负值，特别是例如CTR场景下大部分标签都为0的场景下更容易出现这种情况。</p><h2 id="2020-7-21"><a href="#2020-7-21" class="headerlink" title="2020.7.21"></a>2020.7.21</h2><p>　　丢了几个缺失比较大的特征，对数据做了简单随机采样之后跑lgb5折交了一发，线上分数能到0.7，比预想中的要好，还有一定的提升空间，下一步打算从模型角度切入。</p><h2 id="2020-7-22"><a href="#2020-7-22" class="headerlink" title="2020.7.22"></a>2020.7.22</h2><h3 id="error-Only-one-class-present-in-y-true"><a href="#error-Only-one-class-present-in-y-true" class="headerlink" title="error:Only one class present in y_true"></a>error:Only one class present in y_true</h3><p>　　DeepFM训练过程报错：Only one class present in y_true. ROC AUC score is not defined in that case.<br>　　定义的AUROC函数如下：<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">auroc</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line"><span class="keyword">return</span> tf.compat.v1.py_func(roc_auc_score, (y_true, y_pred), tf.double)</span><br></pre></td></tr></table></figure><br>　　AUC（ROC 曲线下的面积）需要足够数量的任一类才能有意义，而CTR样本中本身就存在着非常严重的正负样本不平衡的问题。<br>　　目前解决方案如下：<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># AUC for a binary classifier</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">auc</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    ptas = tf.stack([binary_PTA(y_true, y_pred, k) <span class="keyword">for</span> k <span class="keyword">in</span> np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">1000</span>)], axis=<span class="number">0</span>)</span><br><span class="line">    pfas = tf.stack([binary_PFA(y_true, y_pred, k) <span class="keyword">for</span> k <span class="keyword">in</span> np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">1000</span>)], axis=<span class="number">0</span>)</span><br><span class="line">    pfas = tf.concat([tf.ones((<span class="number">1</span>,)), pfas], axis=<span class="number">0</span>)</span><br><span class="line">    binSizes = -(pfas[<span class="number">1</span>:] - pfas[:<span class="number">-1</span>])</span><br><span class="line">    s = ptas * binSizes</span><br><span class="line">    <span class="keyword">return</span> K.sum(s, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># PFA, prob false alert for binary classifier</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_PFA</span><span class="params">(y_true, y_pred, threshold=K.variable<span class="params">(value=<span class="number">0.5</span>)</span>)</span>:</span></span><br><span class="line">    y_pred = K.cast(y_pred &gt;= threshold, <span class="string">'float32'</span>)</span><br><span class="line">    <span class="comment"># N = total number of negative labels</span></span><br><span class="line">    N = K.sum(<span class="number">1</span> - y_true)</span><br><span class="line">    <span class="comment"># FP = total number of false alerts, alerts from the negative class labels</span></span><br><span class="line">    FP = K.sum(y_pred - y_pred * y_true)</span><br><span class="line">    <span class="keyword">return</span> FP / N</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># P_TA prob true alerts for binary classifier</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_PTA</span><span class="params">(y_true, y_pred, threshold=K.variable<span class="params">(value=<span class="number">0.5</span>)</span>)</span>:</span></span><br><span class="line">    y_pred = K.cast(y_pred &gt;= threshold, <span class="string">'float32'</span>)</span><br><span class="line">    <span class="comment"># P = total number of positive labels</span></span><br><span class="line">    P = K.sum(y_true)</span><br><span class="line">    <span class="comment"># TP = total number of correct alerts, alerts from the positive class labels</span></span><br><span class="line">    TP = K.sum(y_pred * y_true)</span><br><span class="line">    <span class="keyword">return</span> TP / P</span><br></pre></td></tr></table></figure><br>　　也可在整个训练过程完成后，在vaild_set上计算auc。<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred_ans_val = model.predict(vaild_model_input, batch_size=<span class="number">512</span>)</span><br><span class="line">print(<span class="string">'val_auc'</span>, roc_auc_score(vaild[target].values, pred_ans_val))</span><br></pre></td></tr></table></figure></p><h2 id="2020-7-24"><a href="#2020-7-24" class="headerlink" title="2020.7.24"></a>2020.7.24</h2><h3 id="DeepFM参数调整"><a href="#DeepFM参数调整" class="headerlink" title="DeepFM参数调整"></a>DeepFM参数调整</h3><p>　　* <strong>Regression</strong><br>　　This implementation also supports regression task. To use DeepFM for regression, you can set loss_type as mse. Accordingly, you should use eval_metric for regression, e.g., mse or mae.*<br>　　DeepFM中task参数调整为regression后，loss也需随之进行更改。<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = DeepFM(</span><br><span class="line">linear_feature_columns=linear_feature_columns,</span><br><span class="line">dnn_feature_columns=dnn_feature_columns,</span><br><span class="line">task=<span class="string">'regression'</span>,</span><br><span class="line">l2_reg_embedding=<span class="number">1e-5</span></span><br><span class="line">  )</span><br><span class="line">  </span><br><span class="line">  model.compile(</span><br><span class="line">  <span class="string">'adam'</span>,</span><br><span class="line">  <span class="string">'mse'</span>,</span><br><span class="line">  metrics=[<span class="string">'accuracy'</span>]</span><br><span class="line">      )</span><br></pre></td></tr></table></figure></p><h2 id="2020-7-29"><a href="#2020-7-29" class="headerlink" title="2020.7.29"></a>2020.7.29</h2><h3 id="onehot编码"><a href="#onehot编码" class="headerlink" title="onehot编码"></a>onehot编码</h3><p>　　OneHotEncoder 的输入为 2-D array，data[feat] 返回的 Series 为 1-D array。<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> cat_features:</span><br><span class="line">ohe = OneHotEncoder()</span><br><span class="line">data[feat] = ohe.fit_transform(data[[feat]])</span><br></pre></td></tr></table></figure><br>　　将data[feat]改为data[[feat]]</p><h2 id="2020-8-14"><a href="#2020-8-14" class="headerlink" title="2020.8.14"></a>2020.8.14</h2><h3 id="Error-Input-contains-NaN"><a href="#Error-Input-contains-NaN" class="headerlink" title="Error:Input contains NaN"></a>Error:Input contains NaN</h3><p>　　报错<em>ValueError: Input contains NaN, infinity or a value too large for dtype(‘float32’).</em><br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> tr_x.columns:</span><br><span class="line"><span class="keyword">if</span>(df[index].isna().T.any()):</span><br><span class="line">  print(index,df[index].isna())</span><br><span class="line"><span class="comment"># ------</span></span><br><span class="line">print(df[df.isnull().T.any()])</span><br></pre></td></tr></table></figure><br>　　若检查dataframe无空值后仍然报错。可尝试检查dataframe索引是否连续。<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></p><h2 id="2020-8-15"><a href="#2020-8-15" class="headerlink" title="2020.8.15"></a>2020.8.15</h2><h3 id="pandas-apply设置进度条"><a href="#pandas-apply设置进度条" class="headerlink" title="pandas apply设置进度条"></a>pandas apply设置进度条</h3><p>　　apply速度较慢，可设置进度条实时显示处理进度。<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">tqdm.pandas(desc=<span class="string">'pandas bar'</span>)</span><br><span class="line"></span><br><span class="line">test[<span class="string">'B'</span>] = test.progress_apply(<span class="keyword">lambda</span> x:func(x[<span class="string">'A'</span>]), axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;2020-7-16&quot;&gt;&lt;a href=&quot;#2020-7-16&quot; class=&quot;headerlink&quot; title=&quot;2020.7.16&quot;&gt;&lt;/a&gt;2020.7.16&lt;/h2&gt;&lt;p&gt;　　7.20放赛题数据，先拿kaggle五年前的Click-Through Rate
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hexo+Nginx搭建静态页面博客</title>
    <link href="http://meurice.xyz/2020/ckg5cnsio0002jklx37qy3ycs/"/>
    <id>http://meurice.xyz/2020/ckg5cnsio0002jklx37qy3ycs/</id>
    <published>2020-07-15T07:54:00.000Z</published>
    <updated>2020-07-16T10:32:32.590Z</updated>
    
    <content type="html"><![CDATA[<p>Hexo 是一个快速、简洁且高效的博客框架，且支持 Markdown 语法。</p><p>• 本地环境配置：Node.js+Git+Hexo <br><br>• ECS环境配置：(CentOs 8) Node.js+Git+Pm2+Nginx <br><br>• 安全组配置：阿里云ECS <br><br>• 域名：腾讯云域名解析 <br><br>• Webhook：Github</p><h3 id="本地环境配置"><a href="#本地环境配置" class="headerlink" title="本地环境配置 "></a>本地环境配置 <br></h3><h4 id="Node-js"><a href="#Node-js" class="headerlink" title="Node.js "></a>Node.js <br></h4><p>　Node.js官网<a href="https://nodejs.org/en/" target="_blank" rel="noopener">Node.js</a>，安装目录尽量不要包括空格，命令行下<code>node -v</code>验证是否安装成功。<br><br>　或通过<a href="https://npm.taobao.org/mirrors/node" target="_blank" rel="noopener">淘宝npm镜像</a>安装。</p><h4 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git<br></h4><h5 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h5><p>　Git官网 <a href="https://git-scm.com/" target="_blank" rel="noopener">Git</a>，命令行<code>git --version</code>验证。</p><h5 id="SSH配置"><a href="#SSH配置" class="headerlink" title="SSH配置"></a>SSH配置</h5><p>　　配置Github用户名 <br><br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;example&quot;</span><br><span class="line">git config --global user.email &quot;example@email.com&quot;</span><br></pre></td></tr></table></figure><br>　　生成秘钥<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -b 4096 -C &quot;example@email.com&quot;</span><br></pre></td></tr></table></figure></p><p>　　~/.ssh文件夹下生成id_rsa.pub公有密钥，依次进入Github——Settings——SSH and GPG keys，添加SSH key，将d_rsa.pub中的内容放入Key中。具体可以参考<a href="https://blog.csdn.net/playboyanta123/article/details/49611873?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase" target="_blank" rel="noopener">这篇文章</a>，Linux系统可以参考<a href="https://blog.csdn.net/qq_36663951/article/details/78749217?utm_medium=distribute.pc_relevant.none-task-blog-baidujs-1" target="_blank" rel="noopener">这篇文章</a>。</p><h4 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo<br></h4><p>　　npm全局安装<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install -g hexo-cli</span><br></pre></td></tr></table></figure></p><p>　　验证<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo -version</span><br></pre></td></tr></table></figure></p><p>　　新建文件夹用来存放Hexo代码，在该文件夹下执行命令行。<br>　　初始化Hexo，生成相关文件。<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo -init</span><br></pre></td></tr></table></figure></p><p>　　安装相关依赖<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br></pre></td></tr></table></figure></p><p>　　预览效果<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure><br>　　浏览器进入<a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a></p><h4 id="发布到Github"><a href="#发布到Github" class="headerlink" title="发布到Github"></a>发布到Github<br></h4><p>　　安装git部署工具<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><br>　　修改_config.yml，repo字段修改为github仓库的SSH链接。<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">type: git</span><br><span class="line">repo:git@github.com:egname&#x2F;egrepo.git</span><br><span class="line">branch: master</span><br></pre></td></tr></table></figure><br>　　代码上传至Github<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure></p><h3 id="ECS环境配置"><a href="#ECS环境配置" class="headerlink" title="ECS环境配置 "></a>ECS环境配置 <br></h3><p>　　服务器为阿里云ECS云服务器，CentOS 8。</p><h4 id="Node-js-1"><a href="#Node-js-1" class="headerlink" title="Node.js"></a>Node.js<br></h4><p>　　采用yum方式安装<br>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl -sL https:&#x2F;&#x2F;rpm.nodesource.com&#x2F;setup_10.x | bash -</span><br><span class="line">yum install -y nodejs</span><br><span class="line"></span><br><span class="line">node -v # 验证</span><br></pre></td></tr></table></figure></p><h4 id="Git-1"><a href="#Git-1" class="headerlink" title="Git "></a>Git <br></h4>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install git</span><br><span class="line"></span><br><span class="line">git --version # 验证</span><br></pre></td></tr></table></figure><h5 id="SSH配置-1"><a href="#SSH配置-1" class="headerlink" title="SSH配置"></a>SSH配置<br></h5>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;example&quot;</span><br><span class="line">git config --global user.email &quot;example@email.com&quot;</span><br><span class="line"></span><br><span class="line">ssh-keygen -t rsa -b 4096 -C &quot;example@email.com&quot;</span><br></pre></td></tr></table></figure><p>　　复制公有密钥，在Github上添加新的SSH key，具体可参考上文本地配置。<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~&#x2F;.ssh</span><br><span class="line">cat id_rsa.pub</span><br></pre></td></tr></table></figure><br>　　从Github仓库中克隆代码。<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;</span><br><span class="line">mkdir www</span><br><span class="line"> </span><br><span class="line">cd www</span><br><span class="line">mkdir blog</span><br><span class="line"></span><br><span class="line">cd blog</span><br><span class="line">git clone git@github.com:egname&#x2F;egrepo.git</span><br></pre></td></tr></table></figure></p><h4 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx "></a>Nginx <br></h4><p>　　安装EPEL存储库<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install epel-release</span><br></pre></td></tr></table></figure></p><p>　　安装Nginx<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install nginx</span><br></pre></td></tr></table></figure><br>　　启动Nginx，设置自启动<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl start nginx</span><br><span class="line">sudo systemctl enable nginx</span><br></pre></td></tr></table></figure></p><h5 id="Nginx配置"><a href="#Nginx配置" class="headerlink" title="Nginx配置"></a>Nginx配置</h5><p>　　进入etc/nginx文件夹下的nginx.conf<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;nginx&#x2F;nginx.conf</span><br></pre></td></tr></table></figure><br>　　修改配置文件<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">  listen 80;</span><br><span class="line">  server_name www.example.com;</span><br><span class="line">  root &#x2F;www&#x2F;blog&#x2F;example;</span><br><span class="line">  include &#x2F;etc&#x2F;nginx&#x2F;default.d&#x2F;*.conf;</span><br><span class="line">  </span><br><span class="line">  location &#x2F; &#123;</span><br><span class="line">    root &#x2F;www&#x2F;blog&#x2F;example;</span><br><span class="line">    index index.jsp index.html index.htm;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>　　重启Nginx<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart nginx</span><br></pre></td></tr></table></figure></p><h3 id="安全组配置"><a href="#安全组配置" class="headerlink" title="安全组配置"></a>安全组配置</h3><p>　　以阿里云ECS为例。<br>　　进入控制台——网络与安全——安全组，添加入方向规则。<br>　　添加端口范围分别为80/80（HTTP），443/443（HTTPS），7777/7777（Webhook），授权对象均为0.0.0.0/0。</p><h3 id="域名解析"><a href="#域名解析" class="headerlink" title="域名解析"></a>域名解析</h3><p>　　以腾讯云为例。<br>　　DNS 解析 DNSPod——域名解析列表——选择域名——添加记录——快速添加网站解析——指定服务器主机IP（公网）。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Hexo 是一个快速、简洁且高效的博客框架，且支持 Markdown 语法。&lt;/p&gt;
&lt;p&gt;• 本地环境配置：Node.js+Git+Hexo &lt;br&gt;&lt;br&gt;• ECS环境配置：(CentOs 8) Node.js+Git+Pm2+Nginx &lt;br&gt;&lt;br&gt;• 安全组配
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://meurice.xyz/2020/ckg5cnsis0005jklx0e7gevj7/"/>
    <id>http://meurice.xyz/2020/ckg5cnsis0005jklx0e7gevj7/</id>
    <published>2020-07-15T04:29:57.340Z</published>
    <updated>2020-07-15T04:29:57.340Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
