<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>特征工程 | blog of meurice</title><meta name="description" content="前言　　　　数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。 特征工程　　特征工程是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用，简单来说，就是通过X，创造新的X’，目的是去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系，其本质是一个表示和展现数据的过程。基本的操作包括，衍生（升维），筛选（降维）等。　　例如某分类器接收身"><meta name="author" content="meurice"><meta name="copyright" content="meurice"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://meurice.xyz/2020/ckg5cnu3c000bjklxaclk5yp7/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="特征工程"><meta property="og:url" content="http://meurice.xyz/2020/ckg5cnu3c000bjklxaclk5yp7/"><meta property="og:site_name" content="blog of meurice"><meta property="og:description" content="前言　　　　数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。 特征工程　　特征工程是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用，简单来说，就是通过X，创造新的X’，目的是去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系，其本质是一个表示和展现数据的过程。基本的操作包括，衍生（升维），筛选（降维）等。　　例如某分类器接收身"><meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><meta property="article:published_time" content="2020-07-18T03:42:42.000Z"><meta property="article:modified_time" content="2020-09-06T12:23:12.169Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="prev" title="[学习日志]2020 DIGIX全球校园AI算法精英大赛——赛道B" href="http://meurice.xyz/2020/ckg5cnu3a000ajklx7wih8vpi/"><link rel="next" title="数据预处理——归一化与标准化" href="http://meurice.xyz/2020/ckg5cnsit0006jklxfxkd5ken/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="blog of meurice" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">12</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言　　</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#特征工程"><span class="toc-number">2.</span> <span class="toc-text">特征工程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据预处理"><span class="toc-number">3.</span> <span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#单特征"><span class="toc-number">3.1.</span> <span class="toc-text">单特征</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#标准化与归一化"><span class="toc-number">3.1.1.</span> <span class="toc-text">标准化与归一化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#缺失值"><span class="toc-number">3.1.2.</span> <span class="toc-text">缺失值</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#均值-中位数-众数-固定值填充"><span class="toc-number">3.1.2.1.</span> <span class="toc-text">均值&#x2F;中位数&#x2F;众数&#x2F;固定值填充</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#建模预测"><span class="toc-number">3.1.2.2.</span> <span class="toc-text">建模预测</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#高维映射"><span class="toc-number">3.1.2.3.</span> <span class="toc-text">高维映射</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#其他"><span class="toc-number">3.1.2.4.</span> <span class="toc-text">其他</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#特征二值化"><span class="toc-number">3.1.3.</span> <span class="toc-text">特征二值化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#哑编码-独热编码"><span class="toc-number">3.1.4.</span> <span class="toc-text">哑编码&#x2F;独热编码</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#哑编码-dummy-encoding"><span class="toc-number">3.1.4.1.</span> <span class="toc-text">哑编码(dummy encoding)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#独热编码-one-hot-encoding"><span class="toc-number">3.1.4.2.</span> <span class="toc-text">独热编码(one-hot encoding)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#多特征"><span class="toc-number">3.2.</span> <span class="toc-text">多特征</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#特征选择"><span class="toc-number">3.2.1.</span> <span class="toc-text">特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#特征选择原理"><span class="toc-number">3.2.1.1.</span> <span class="toc-text">特征选择原理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Filter"><span class="toc-number">3.2.1.2.</span> <span class="toc-text">Filter</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#方差选择法"><span class="toc-number">3.2.1.2.1.</span> <span class="toc-text">方差选择法</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#相关系数法"><span class="toc-number">3.2.1.2.2.</span> <span class="toc-text">相关系数法</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#卡方检验"><span class="toc-number">3.2.1.2.3.</span> <span class="toc-text">卡方检验</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#互信息法"><span class="toc-number">3.2.1.2.4.</span> <span class="toc-text">互信息法</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Wrapper"><span class="toc-number">3.2.1.3.</span> <span class="toc-text">Wrapper</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#递归特征消除法"><span class="toc-number">3.2.1.3.1.</span> <span class="toc-text">递归特征消除法</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Embedded"><span class="toc-number">3.2.1.4.</span> <span class="toc-text">Embedded</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#基于惩罚项的特征选择法"><span class="toc-number">3.2.1.4.1.</span> <span class="toc-text">基于惩罚项的特征选择法</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#基于树模型的特征选择法"><span class="toc-number">3.2.1.4.2.</span> <span class="toc-text">基于树模型的特征选择法</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">blog of meurice</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">特征工程</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="Created 2020-07-18 11:42:42"><i class="far fa-calendar-alt fa-fw"></i> Created 2020-07-18</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="Updated 2020-09-06 20:23:12"><i class="fas fa-history fa-fw"></i> Updated 2020-09-06</span></time></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta__icon"></i><span>Word count:</span><span class="word-count">2.7k</span><span class="post-meta__separator">|</span><i class="far fa-clock fa-fw post-meta__icon"></i><span>Reading time: 8 min</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"></span><span class="post-meta-commentcount"></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h2 id="前言"><a href="#前言" class="headerlink" title="前言　　"></a>前言　　</h2><p>　　数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。</p>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>　　特征工程是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用，简单来说，就是通过X，创造新的X’，目的是去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系，其本质是一个表示和展现数据的过程。基本的操作包括，衍生（升维），筛选（降维）等。<br>　　例如某分类器接收身高、体重两个参数来判断这个人是否肥胖，仅通过体重无法判断某个人的胖瘦，对于该例，一个非常经典的特征工程是，BMI指数，BMI=体重/(身高^2)，通过BMI指数，可以清晰地对一个人的胖瘦进行刻画。  </p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>　　常见的数据可分为结构化数据（例如关系型数据库的表）和非结构化数据（文本、图像、音频、视频等）。</p>
<h3 id="单特征"><a href="#单特征" class="headerlink" title="单特征"></a>单特征</h3><h4 id="标准化与归一化"><a href="#标准化与归一化" class="headerlink" title="标准化与归一化"></a>标准化与归一化</h4><p>　　该部分可以参考<a href="http://meurice.xyz/2020/ckcqevh3t0004xclxakyx24ma/">数据预处理——归一化与标准化</a>。</p>
<h4 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h4><h5 id="均值-中位数-众数-固定值填充"><a href="#均值-中位数-众数-固定值填充" class="headerlink" title="均值/中位数/众数/固定值填充"></a>均值/中位数/众数/固定值填充</h5><p>　　如果样本属性的距离是可度量的，则使用该属性有效值的平均值来补全；如果样本属性的距离不可度量，则可以采用众数或者中位数来补全。<br>　　或可根据某一特征对样本进行分类/聚合后（例如船运GPS数据，根据运单号进行聚合后，对样本数据缺失值进行填充），根据同类其他样本该属性的均值补全缺失值，同上述方法类似。<br>　　对于缺失值也可以采用固定的数值来进行填充。</p>
<h5 id="建模预测"><a href="#建模预测" class="headerlink" title="建模预测"></a>建模预测</h5><p>　　将缺失值字段作为预测对象，建立模型对其进行预测，根据该模型补全原训练集的缺失值。这个方法根本的缺陷是如果其他属性和缺失属性无关，则预测的结果毫无意义；但若模型对预测字段拟合效果相当好，则说明这个缺失属性没必要纳入数据集；一般的情况是介于两者之间。</p>
<h5 id="高维映射"><a href="#高维映射" class="headerlink" title="高维映射"></a>高维映射</h5><p>　　将属性映射到高维空间，采用独热码编码（one-hot）技术。将包含 K 个离散取值范围的属性值扩展为 K+1 个属性值，若该属性值缺失，则扩展后的第 K+1 个属性值置为 1。<br>　　这种做法既保留了所有的信息，也未添加任何额外信息，但会增加数据的维度，增大了计算量，一般在样本量非常大时效果才比较好。</p>
<h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><p>　　如多重插补、压缩感知和矩阵补全等，此处不具体展开，可以参考<a href="https://mp.weixin.qq.com/s/BnTXjzHSb5-4s0O0WuZYlg" target="_blank" rel="noopener">这篇文章</a>。</p>
<h4 id="特征二值化"><a href="#特征二值化" class="headerlink" title="特征二值化"></a>特征二值化</h4><p>　　 设立阈值，将特征二值化。<br>　　<img src= "/img/loading.gif" data-src="https://wx1.sbimg.cn/2020/07/18/ClGSk.png" alt="erzhihua"><br>　　可以类比将模拟信号转换成数字信号过程中的量化。<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_ = preprocessing.Binarizer(threshold=<span class="number">0</span>).transform(X)</span><br></pre></td></tr></table></figure></p>
<h4 id="哑编码-独热编码"><a href="#哑编码-独热编码" class="headerlink" title="哑编码/独热编码"></a>哑编码/独热编码</h4><p>　　哑编码/独热编码针对定性的特征进行处理。</p>
<h5 id="哑编码-dummy-encoding"><a href="#哑编码-dummy-encoding" class="headerlink" title="哑编码(dummy encoding)"></a>哑编码(dummy encoding)</h5><p>　　假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。<br>　　例如描述一个人的身材，我们可以用偏瘦、正常、偏胖，这些描述词经过哑编码就会得到：<br>　　　　偏廋 —&gt; [1, 0, 0]<br>　　　　正常 —&gt; [0, 1, 0]<br>　　　　偏胖 —&gt; [0, 0, 1]<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_ = pd.Categorical(df[<span class="string">'c'</span>]).codes</span><br></pre></td></tr></table></figure></p>
<h5 id="独热编码-one-hot-encoding"><a href="#独热编码-one-hot-encoding" class="headerlink" title="独热编码(one-hot encoding)"></a>独热编码(one-hot encoding)</h5><p>　　同上例，实际用2个状态位就足够反应上述3个类别的信息：<br>　　　　偏廋 —&gt; [1, 0]<br>　　　　正常 —&gt; [0, 1]<br>　　　　偏胖 —&gt; [0, 0]<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">encoder=OneHotEncoder(sparse=<span class="literal">False</span>) </span><br><span class="line"><span class="comment"># sparse是一个布尔值，指定结果是否稀疏。</span></span><br><span class="line"><span class="comment"># 若sparse=True，则每个样本的独热码为一个稀疏矩阵。</span></span><br></pre></td></tr></table></figure><br>  <br><br>　　关于哑编码/独热编码的区别和联系以及连续值的离散化提升模型的非线性能力的原因，可以参考<a href="https://www.cnblogs.com/lianyingteng/p/7792693.html" target="_blank" rel="noopener">这篇文章</a>。</p>
<h3 id="多特征"><a href="#多特征" class="headerlink" title="多特征"></a>多特征</h3><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p>　　数据预处理完成后，需要选择有意义的特征输入机器学习的算法和模型进行训练，一般从以下两个方面考虑：<br>　　· 特征是否发散（某特征不发散，说明对于区分样本作用并不大）<br>　　· 特征与目标的相关性  </p>
<p>　　特征选择主要包括：<br>　　· Filter Method （过滤式）<br>　　· Wrapper Method （包装式）<br>　　· Embedded Method （嵌入式）</p>
<h5 id="特征选择原理"><a href="#特征选择原理" class="headerlink" title="特征选择原理"></a>特征选择原理</h5><p>　　·去除无关特征可以降低学习任务的难度，也同样让模型变得简单，降低计算复杂度
　　</p>
<h5 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h5><p>　　过滤式方法先对数据集进行特征选择，然后再训练模型，<strong>特征选择过程与后续模型训练无关</strong>。<br>　　通过统计学的方法对每个feature给出一个score，通过score对特征进行排序，然后选取score最高的子集.。这种方法仅仅对每个feature进行<strong>独立考虑</strong>，没有考虑到feture之间的依赖性或相关性。  </p>
<h6 id="方差选择法"><a href="#方差选择法" class="headerlink" title="方差选择法"></a>方差选择法</h6><p>　　计算各个特征的方差，根据阈值，<strong>选择方差大于阈值的特征</strong>。即若样本中该特征差异并不大，则认为该特征对于区分样本贡献不大，故可以将其去掉。<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"></span><br><span class="line">VarianceThreshold(threshold=<span class="number">0</span>).fit_transform(data)</span><br></pre></td></tr></table></figure></p>
<h6 id="相关系数法"><a href="#相关系数法" class="headerlink" title="相关系数法"></a>相关系数法</h6><p>　　计算各个特征对目标值的相关系数以及相关系数的P值。<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"></span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(map(<span class="keyword">lambda</span> x:pearsonr(x, Y), X.T)).T, </span><br><span class="line">            	k=<span class="number">4</span>).fit_transform(data, target)</span><br><span class="line"><span class="comment"># 第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。（在此定义为计算相关系数）</span></span><br><span class="line"><span class="comment"># 参数k为选择的特征个数，选择k个最好的特征，返回选择特征后的数据</span></span><br></pre></td></tr></table></figure></p>
<h6 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h6><p>　　经典的卡方检验是<strong>检验定性自变量对定性因变量的相关性</strong>，是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，如果卡方值越大，二者偏差程度越大；反之，二者偏差越小；若两个值完全相等时，卡方值就为0，表明理论值完全符合。<br>　　假设自变量有N种取值，因变量有M种取值，考虑自变量等于 i 且因变量等于 j 的样本频数的观察值与期望的差距。<br>　　<img src= "/img/loading.gif" data-src="https://wx1.sbimg.cn/2020/07/19/Cyng1.png" alt="x2"><br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2 </span><br><span class="line"><span class="comment"># 选择k个最佳特征</span></span><br><span class="line">SelectKBest(chi2, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure></p>
<h6 id="互信息法"><a href="#互信息法" class="headerlink" title="互信息法"></a>互信息法</h6><p>　　互信息(Mutual Information)是信息论里一种有用的信息度量，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。<br>　　经典的互信息<strong>评价定性自变量对定性因变量的相关性</strong>。<br>　　设两个随机变量(X, Y)的联合分布为p(x, y)，边缘分布分别为p(x), p(y)，互信息I(X, Y)是联合分布p(x, y)与边缘分布p(x)p(y)的相对熵，即：<br><img src= "/img/loading.gif" data-src="https://wx1.sbimg.cn/2020/07/19/CV0Ek.png" alt="mutual info"><br> 　　关系图：<br><img src= "/img/loading.gif" data-src="https://wx2.sbimg.cn/2020/07/19/CVofa.png" alt="mutual"><br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> minepy <span class="keyword">import</span> MINE</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义mic方法将MINE设为函数式的，返回一个二元组，二元组的第2 项设置成固定的P值0.5</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mic</span><span class="params">(x, y)</span>:</span></span><br><span class="line">	m = MINE()</span><br><span class="line">  m.compute_score(x, y)</span><br><span class="line">  <span class="keyword">return</span> (m.mic(), <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(map(<span class="keyword">lambda</span> x:mic(x, Y), X.T)).T,</span><br><span class="line">				k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure></p>
<h5 id="Wrapper"><a href="#Wrapper" class="headerlink" title="Wrapper"></a>Wrapper</h5><p>　　包裹式特征选择直接把最终将要使用的模型的性能作为特征子集的评价标准，即包裹式特征选择的目的就是为给定的模型选择最有利于其性能的特征子集。从最终模型的性能来看，包裹式特征选择比过滤式特征选择更好，但需要多次训练模型，计算开销较大。<br><img src= "/img/loading.gif" data-src="https://wx2.sbimg.cn/2020/07/19/CVCpn.png" alt="filter mutual"></p>
<h6 id="递归特征消除法"><a href="#递归特征消除法" class="headerlink" title="递归特征消除法"></a>递归特征消除法</h6><p>　　递归特征消除法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处选择LR为基模型(estimator)</span></span><br><span class="line">RFE(estimator=LogisticRegression(), n_features_to_select=<span class="number">4</span>).fit_transform(data, target)</span><br></pre></td></tr></table></figure></p>
<h5 id="Embedded"><a href="#Embedded" class="headerlink" title="Embedded"></a>Embedded</h5><p>　　在前两种特征选择方法中，特征选择过程和模型训练过程是有明显分别的两个过程。嵌入式特征选择是<strong>将特征选择过程与学习器训练过程融为一体</strong>，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。例如岭回归(Ridge)、LASSO回归。常利用正则化，如L1，L2范数，主要应用于如线性回归、逻辑回归以及支持向量机(SVM)等算法；使用决策树思想，包括决策树、随机森林、Gradient Boosting 等。<br>　　若使用L2范数正则化，则此时优化目标的公式即为岭回归(ridge regression)，若是L1范数正则化，则是LASSO回归(Least Absolute Shrinkage and Selection Operator)。L1范数和L2范数正则化都有助于降低过拟合风险，但前者还会带来一个额外的好处，它比后者更易于获得稀疏解，即它求得的w会有更少的非零分类。换言之，采用L1范数比L2范数更易于得到稀疏解。（参考<a href="https://zhuanlan.zhihu.com/p/120924870" target="_blank" rel="noopener">机器学习（六）：特征选择方法—Filter,Wrapper,Embedded</a>）</p>
<h6 id="基于惩罚项的特征选择法"><a href="#基于惩罚项的特征选择法" class="headerlink" title="基于惩罚项的特征选择法"></a>基于惩罚项的特征选择法</h6><p>　　使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。<br>　　带L1惩罚项的LR：<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import SelectFromModel</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"> </span><br><span class="line">SelectFromModel(LogisticRegression(penalty&#x3D;&quot;l1&quot;, C&#x3D;0.1)).fit_transform(data, target)</span><br></pre></td></tr></table></figure></p>
<h6 id="基于树模型的特征选择法"><a href="#基于树模型的特征选择法" class="headerlink" title="基于树模型的特征选择法"></a>基于树模型的特征选择法</h6><p>　　GBDT作为基模型<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"></span><br><span class="line">SelectFromModel(GradientBoostingClassifier()).fit_transform(data, target)</span><br></pre></td></tr></table></figure></p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">meurice</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://meurice.xyz/2020/ckg5cnu3c000bjklxaclk5yp7/">http://meurice.xyz/2020/ckg5cnu3c000bjklxaclk5yp7/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/ckg5cnu3a000ajklx7wih8vpi/"><img class="prev-cover" data-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">[学习日志]2020 DIGIX全球校园AI算法精英大赛——赛道B</div></div></a></div><div class="next-post pull-right"><a href="/2020/ckg5cnsit0006jklxfxkd5ken/"><img class="next-cover" data-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">数据预处理——归一化与标准化</div></div></a></div></nav></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By meurice</div><div class="framework-info"><span>Driven </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="font_plus" title="Increase Font Size"><i class="fas fa-plus"></i></button><button id="font_minus" title="Decrease Font Size"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="Switch Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script id="ribbon_piao" mobile="true" src="/js/third-party/piao.js"></script><script src="/js/third-party/activate-power-mode.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
document.body.addEventListener('input', POWERMODE);
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body></html>